{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace239e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# from sklearn import svm\n",
    "from keras.layers import Input, Dense, ZeroPadding2D, Dropout, Activation\n",
    "from keras.layers import Input, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from numba import cuda\n",
    "# from sklearn import datasets\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%run Dataset.ipynb\n",
    "# inputShape = (10,10,9)\n",
    "inputShape = (37,5,9)\n",
    "Ratio=\"R4peopleDataset\"  #R4 or R1C\n",
    "Pnumber=15\n",
    "DsetName=str(Pnumber) + Ratio\n",
    "data_train,data_test,label_trian,label_test = GetNew(inputShape,DsetName)\n",
    "# data_train = data_train.reshape(np.shape(data_train)[0],100,1,9)\n",
    "# data_test = data_test.reshape(np.shape(data_test)[0],100,1,9)\n",
    "print(\"data loaded\")\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def conv2d_bn(inpt, filters=64, kernel_size=(3, 3), strides=1, padding='same'):\n",
    "    x = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(inpt)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def basic_bottle(inpt, filters=64, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False):\n",
    "    x = conv2d_bn(inpt, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "    x = conv2d_bn(x, filters=filters)\n",
    "#     x = layers.Dropout(0.1)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if if_baisc == True:\n",
    "        temp = conv2d_bn(inpt, filters=filters, kernel_size=(1, 1), strides=2, padding='same')\n",
    "        outt = layers.add([x, temp])\n",
    "    else:\n",
    "        outt = layers.add([x, inpt])\n",
    "    return outt\n",
    "\n",
    "\n",
    "def EMGNet_block(inpt, filters=64, kernel_size=(3, 3), strides=1, padding='same'):\n",
    "    x = layers.Conv2D(filters=filters, kernel_size=(6, 2), strides=strides, padding='valid')(inpt)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x_shortcut = x\n",
    "    x = conv2d_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "    x = conv2d_bn(x, filters=filters, kernel_size=kernel_size)\n",
    "    x = layers.add([x, x_shortcut])\n",
    "    x_shortcut = x\n",
    "    x = conv2d_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)\n",
    "    x = conv2d_bn(x, filters=filters, kernel_size=kernel_size)\n",
    "    x = layers.add([x, x_shortcut])\n",
    "    return x\n",
    "\n",
    "\n",
    "def CNN(input_shape=inputShape, classes=10):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = conv2d_bn(X_input, filters=64, kernel_size=(4, 4), strides=1, padding='valid') # 7 7 64\n",
    "\n",
    "    # layer 2\n",
    "    X = basic_bottle(X, filters=64, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)#7 7 64\n",
    "    X = basic_bottle(X, filters=64, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)#7 7 64\n",
    "    X = layers.Dropout(.2)(X)\n",
    "    # layer 3\n",
    "    X = basic_bottle(X, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True) #\n",
    "    X = basic_bottle(X, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n",
    "    X = layers.Dropout(.2)(X)\n",
    "    # layer 4\n",
    "    X = basic_bottle(X, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n",
    "    X = basic_bottle(X, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n",
    "    X = layers.Dropout(.2)(X)\n",
    "    # layer 5\n",
    "    X = basic_bottle(X, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n",
    "    X = basic_bottle(X, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n",
    "    X = layers.Dropout(.2)(X)\n",
    "    # GlobalAveragePool\n",
    "    X = layers.GlobalAveragePooling2D()(X)\n",
    "    X = layers.Dropout(.5)(X)\n",
    "    X = layers.Dense(classes, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet18')\n",
    "    return model\n",
    "\n",
    "def EMGNet(input_shape=inputShape, classes=10):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = EMGNet_block(X_input, filters=64, kernel_size=(4, 2), strides=1, padding='same')\n",
    "    X = EMGNet_block(X, filters=128, kernel_size=(4, 2), strides=1, padding='same')\n",
    "    X = EMGNet_block(X, filters=256, kernel_size=(4, 2), strides=1, padding='same')\n",
    "    X = EMGNet_block(X, filters=512, kernel_size=(4, 2), strides=1, padding='same')\n",
    "    X = layers.GlobalAveragePooling2D()(X)\n",
    "    X = layers.Dropout(.5)(X)\n",
    "    X = layers.Dense(classes, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='EMGNet')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef29243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EMGNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 37, 5, 9)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 4, 64)    6976        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 4, 64)    256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 4, 64)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 4, 64)    32832       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 4, 64)    256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 4, 64)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 4, 64)    32832       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 4, 64)    256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 4, 64)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 4, 64)    0           activation_33[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 4, 64)    32832       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 4, 64)    256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 4, 64)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 4, 64)    32832       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 4, 64)    256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 4, 64)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 4, 64)    0           activation_35[0][0]              \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 27, 3, 128)   98432       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 27, 3, 128)   512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 27, 3, 128)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 27, 3, 128)   131200      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 27, 3, 128)   512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 27, 3, 128)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 27, 3, 128)   131200      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 27, 3, 128)   512         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 27, 3, 128)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 27, 3, 128)   0           activation_38[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 27, 3, 128)   131200      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 27, 3, 128)   512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 27, 3, 128)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 27, 3, 128)   131200      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 27, 3, 128)   512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 27, 3, 128)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 27, 3, 128)   0           activation_40[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 22, 2, 256)   393472      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 22, 2, 256)   1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 22, 2, 256)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 22, 2, 256)   524544      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 22, 2, 256)   1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 22, 2, 256)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 22, 2, 256)   524544      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 22, 2, 256)   1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 22, 2, 256)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 22, 2, 256)   0           activation_43[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 22, 2, 256)   524544      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 22, 2, 256)   1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 22, 2, 256)   0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 22, 2, 256)   524544      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 22, 2, 256)   1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 22, 2, 256)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 22, 2, 256)   0           activation_45[0][0]              \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 1, 512)   1573376     add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 1, 512)   2048        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 1, 512)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 1, 512)   2097664     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 1, 512)   2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 1, 512)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 1, 512)   2097664     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 1, 512)   2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 1, 512)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 17, 1, 512)   0           activation_48[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 1, 512)   2097664     add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 1, 512)   2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 1, 512)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 1, 512)   2097664     activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 1, 512)   2048        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 1, 512)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 17, 1, 512)   0           activation_50[0][0]              \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5130        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,241,546\n",
      "Trainable params: 13,231,946\n",
      "Non-trainable params: 9,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = svm.SVC(gamma=1, kernel='rbf', C=0.1,  verbose=True, shrinking=True, max_iter=1, coef0=0.0, decision_function_shape='ovo')\n",
    "model = EMGNet(input_shape=inputShape, classes=10)\n",
    "model.summary()\n",
    "# print(np.shape(data_train))\n",
    "# svc = SVC()\n",
    "# o_vs_r = OneVsRestClassifier(svc)\n",
    "# o_vs_r.fit(data_train, label_trian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2713e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2a4df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      " 6/85 [=>............................] - ETA: 13s - loss: 3.4520 - accuracy: 0.3242WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0442s vs `on_train_batch_end` time: 0.1424s). Check your callbacks.\n",
      "85/85 [==============================] - 17s 182ms/step - loss: 1.6928 - accuracy: 0.3915 - val_loss: 21375.4062 - val_accuracy: 0.1000\n",
      "Epoch 2/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 1.0389 - accuracy: 0.5743 - val_loss: 365.9483 - val_accuracy: 0.2075\n",
      "Epoch 3/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.8760 - accuracy: 0.6567 - val_loss: 48.8941 - val_accuracy: 0.3267\n",
      "Epoch 4/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 0.7457 - accuracy: 0.7121 - val_loss: 2.2766 - val_accuracy: 0.5502\n",
      "Epoch 5/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 0.6985 - accuracy: 0.7348 - val_loss: 4.8774 - val_accuracy: 0.5608\n",
      "Epoch 6/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 0.7089 - accuracy: 0.7254 - val_loss: 2.2063 - val_accuracy: 0.5231\n",
      "Epoch 7/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.6016 - accuracy: 0.7679 - val_loss: 0.8878 - val_accuracy: 0.6816\n",
      "Epoch 8/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 0.5684 - accuracy: 0.7802 - val_loss: 1.0904 - val_accuracy: 0.6925\n",
      "Epoch 9/250\n",
      "85/85 [==============================] - 16s 183ms/step - loss: 0.6377 - accuracy: 0.7623 - val_loss: 5.2821 - val_accuracy: 0.5902\n",
      "Epoch 10/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.6070 - accuracy: 0.7659 - val_loss: 0.6293 - val_accuracy: 0.7824\n",
      "Epoch 11/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.5038 - accuracy: 0.8036 - val_loss: 0.5662 - val_accuracy: 0.7976\n",
      "Epoch 12/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.4948 - accuracy: 0.8122 - val_loss: 0.6790 - val_accuracy: 0.7820\n",
      "Epoch 13/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.4632 - accuracy: 0.8238 - val_loss: 0.5483 - val_accuracy: 0.8239\n",
      "Epoch 14/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.4419 - accuracy: 0.8333 - val_loss: 0.5815 - val_accuracy: 0.8102\n",
      "Epoch 15/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.4077 - accuracy: 0.8444 - val_loss: 0.9581 - val_accuracy: 0.7431\n",
      "Epoch 16/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3907 - accuracy: 0.8455 - val_loss: 0.6701 - val_accuracy: 0.7596\n",
      "Epoch 17/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3826 - accuracy: 0.8535 - val_loss: 0.4519 - val_accuracy: 0.8408\n",
      "Epoch 18/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3515 - accuracy: 0.8669 - val_loss: 0.6476 - val_accuracy: 0.8031\n",
      "Epoch 19/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3632 - accuracy: 0.8621 - val_loss: 0.5595 - val_accuracy: 0.8235\n",
      "Epoch 20/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3390 - accuracy: 0.8740 - val_loss: 0.6396 - val_accuracy: 0.8094\n",
      "Epoch 21/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3221 - accuracy: 0.8809 - val_loss: 0.5457 - val_accuracy: 0.8243\n",
      "Epoch 22/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3041 - accuracy: 0.8851 - val_loss: 0.4792 - val_accuracy: 0.8400\n",
      "Epoch 23/250\n",
      "85/85 [==============================] - 14s 169ms/step - loss: 0.2949 - accuracy: 0.8888 - val_loss: 0.6034 - val_accuracy: 0.8318\n",
      "Epoch 24/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2763 - accuracy: 0.8972 - val_loss: 0.8228 - val_accuracy: 0.7753\n",
      "Epoch 25/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2545 - accuracy: 0.9048 - val_loss: 0.5516 - val_accuracy: 0.8176\n",
      "Epoch 26/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2568 - accuracy: 0.9054 - val_loss: 0.4185 - val_accuracy: 0.8745\n",
      "Epoch 27/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2513 - accuracy: 0.9065 - val_loss: 0.4737 - val_accuracy: 0.8620\n",
      "Epoch 28/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2405 - accuracy: 0.9131 - val_loss: 0.6188 - val_accuracy: 0.8467\n",
      "Epoch 29/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2364 - accuracy: 0.9119 - val_loss: 0.4436 - val_accuracy: 0.8655\n",
      "Epoch 30/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2135 - accuracy: 0.9189 - val_loss: 0.5343 - val_accuracy: 0.8369\n",
      "Epoch 31/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2106 - accuracy: 0.9211 - val_loss: 0.4276 - val_accuracy: 0.8776\n",
      "Epoch 32/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2005 - accuracy: 0.9256 - val_loss: 0.5607 - val_accuracy: 0.8361\n",
      "Epoch 33/250\n",
      "85/85 [==============================] - 16s 185ms/step - loss: 0.2151 - accuracy: 0.9207 - val_loss: 0.5724 - val_accuracy: 0.8506\n",
      "Epoch 34/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1937 - accuracy: 0.9289 - val_loss: 0.5102 - val_accuracy: 0.8498\n",
      "Epoch 35/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1801 - accuracy: 0.9317 - val_loss: 0.3901 - val_accuracy: 0.8906\n",
      "Epoch 36/250\n",
      "85/85 [==============================] - 15s 176ms/step - loss: 0.2918 - accuracy: 0.9015 - val_loss: 1512.8008 - val_accuracy: 0.1008\n",
      "Epoch 37/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3095 - accuracy: 0.8982 - val_loss: 3.0065 - val_accuracy: 0.5420\n",
      "Epoch 38/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.8921 - accuracy: 0.7125 - val_loss: 37.7373 - val_accuracy: 0.1996\n",
      "Epoch 39/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.5107 - accuracy: 0.8032 - val_loss: 1.1730 - val_accuracy: 0.6329\n",
      "Epoch 40/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.4090 - accuracy: 0.8449 - val_loss: 0.4428 - val_accuracy: 0.8392\n",
      "Epoch 41/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3389 - accuracy: 0.8745 - val_loss: 0.4490 - val_accuracy: 0.8451\n",
      "Epoch 42/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2969 - accuracy: 0.8914 - val_loss: 0.5848 - val_accuracy: 0.8255\n",
      "Epoch 43/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2641 - accuracy: 0.9038 - val_loss: 0.4429 - val_accuracy: 0.8690\n",
      "Epoch 44/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2701 - accuracy: 0.9083 - val_loss: 0.5084 - val_accuracy: 0.8475\n",
      "Epoch 45/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.5869 - accuracy: 0.8026 - val_loss: 1.1021 - val_accuracy: 0.7486\n",
      "Epoch 46/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3631 - accuracy: 0.8650 - val_loss: 0.4815 - val_accuracy: 0.8337\n",
      "Epoch 47/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2917 - accuracy: 0.8901 - val_loss: 0.4859 - val_accuracy: 0.8322\n",
      "Epoch 48/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2529 - accuracy: 0.9080 - val_loss: 0.7318 - val_accuracy: 0.7922\n",
      "Epoch 49/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2368 - accuracy: 0.9119 - val_loss: 0.5411 - val_accuracy: 0.8357\n",
      "Epoch 50/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2079 - accuracy: 0.9236 - val_loss: 0.4736 - val_accuracy: 0.8686\n",
      "Epoch 51/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2085 - accuracy: 0.9211 - val_loss: 0.5050 - val_accuracy: 0.8816\n",
      "Epoch 52/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1939 - accuracy: 0.9292 - val_loss: 0.4846 - val_accuracy: 0.8820\n",
      "Epoch 53/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1719 - accuracy: 0.9381 - val_loss: 0.4900 - val_accuracy: 0.8878\n",
      "Epoch 54/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1768 - accuracy: 0.9343 - val_loss: 0.4124 - val_accuracy: 0.8898\n",
      "Epoch 55/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1637 - accuracy: 0.9394 - val_loss: 0.4909 - val_accuracy: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1634 - accuracy: 0.9400 - val_loss: 0.4652 - val_accuracy: 0.8980\n",
      "Epoch 57/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1437 - accuracy: 0.9461 - val_loss: 0.3905 - val_accuracy: 0.9106\n",
      "Epoch 58/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1543 - accuracy: 0.9463 - val_loss: 0.7507 - val_accuracy: 0.8584\n",
      "Epoch 59/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1497 - accuracy: 0.9480 - val_loss: 0.4800 - val_accuracy: 0.8918\n",
      "Epoch 60/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1452 - accuracy: 0.9476 - val_loss: 0.5069 - val_accuracy: 0.9000\n",
      "Epoch 61/250\n",
      "85/85 [==============================] - 15s 172ms/step - loss: 0.1362 - accuracy: 0.9508 - val_loss: 0.6528 - val_accuracy: 0.8663\n",
      "Epoch 62/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1374 - accuracy: 0.9506 - val_loss: 0.3828 - val_accuracy: 0.9075\n",
      "Epoch 63/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1303 - accuracy: 0.9513 - val_loss: 0.5287 - val_accuracy: 0.8984\n",
      "Epoch 64/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.4788 - val_accuracy: 0.9027\n",
      "Epoch 65/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1163 - accuracy: 0.9581 - val_loss: 0.4095 - val_accuracy: 0.9173\n",
      "Epoch 66/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1182 - accuracy: 0.9537 - val_loss: 0.4075 - val_accuracy: 0.8992\n",
      "Epoch 67/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1295 - accuracy: 0.9540 - val_loss: 0.5506 - val_accuracy: 0.8761\n",
      "Epoch 68/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1083 - accuracy: 0.9596 - val_loss: 0.4640 - val_accuracy: 0.9169\n",
      "Epoch 69/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1061 - accuracy: 0.9591 - val_loss: 0.5735 - val_accuracy: 0.9024\n",
      "Epoch 70/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1189 - accuracy: 0.9549 - val_loss: 0.3963 - val_accuracy: 0.9012\n",
      "Epoch 71/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1025 - accuracy: 0.9622 - val_loss: 0.4792 - val_accuracy: 0.9059\n",
      "Epoch 72/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1007 - accuracy: 0.9628 - val_loss: 0.4875 - val_accuracy: 0.9094\n",
      "Epoch 73/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0876 - accuracy: 0.9664 - val_loss: 0.4887 - val_accuracy: 0.9157\n",
      "Epoch 74/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0984 - accuracy: 0.9650 - val_loss: 0.6599 - val_accuracy: 0.8878\n",
      "Epoch 75/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0954 - accuracy: 0.9625 - val_loss: 0.6908 - val_accuracy: 0.8769\n",
      "Epoch 76/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0849 - accuracy: 0.9684 - val_loss: 0.5557 - val_accuracy: 0.8992\n",
      "Epoch 77/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0974 - accuracy: 0.9647 - val_loss: 0.4512 - val_accuracy: 0.9149\n",
      "Epoch 78/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0855 - accuracy: 0.9684 - val_loss: 0.5078 - val_accuracy: 0.9200\n",
      "Epoch 79/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.5739 - val_accuracy: 0.8980\n",
      "Epoch 80/250\n",
      "85/85 [==============================] - 16s 187ms/step - loss: 0.0835 - accuracy: 0.9716 - val_loss: 0.5965 - val_accuracy: 0.8847\n",
      "Epoch 81/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0938 - accuracy: 0.9675 - val_loss: 0.6610 - val_accuracy: 0.8753\n",
      "Epoch 82/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0929 - accuracy: 0.9675 - val_loss: 0.5762 - val_accuracy: 0.8886\n",
      "Epoch 83/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0869 - accuracy: 0.9704 - val_loss: 0.6378 - val_accuracy: 0.8957\n",
      "Epoch 84/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0837 - accuracy: 0.9695 - val_loss: 0.5769 - val_accuracy: 0.9071\n",
      "Epoch 85/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0899 - accuracy: 0.9697 - val_loss: 0.4964 - val_accuracy: 0.9106\n",
      "Epoch 86/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0752 - accuracy: 0.9737 - val_loss: 0.4942 - val_accuracy: 0.9055\n",
      "Epoch 87/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0797 - accuracy: 0.9709 - val_loss: 0.4646 - val_accuracy: 0.9176\n",
      "Epoch 88/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.3272 - accuracy: 0.8912 - val_loss: 6.2548 - val_accuracy: 0.7325\n",
      "Epoch 89/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.2165 - accuracy: 0.9302 - val_loss: 0.6153 - val_accuracy: 0.8518\n",
      "Epoch 90/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1274 - accuracy: 0.9543 - val_loss: 0.6126 - val_accuracy: 0.8424\n",
      "Epoch 91/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1032 - accuracy: 0.9605 - val_loss: 0.4355 - val_accuracy: 0.8969\n",
      "Epoch 92/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0754 - accuracy: 0.9729 - val_loss: 0.5722 - val_accuracy: 0.8843\n",
      "Epoch 93/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.1047 - accuracy: 0.9644 - val_loss: 0.4383 - val_accuracy: 0.9086\n",
      "Epoch 94/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0842 - accuracy: 0.9677 - val_loss: 0.4571 - val_accuracy: 0.9114\n",
      "Epoch 95/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0754 - accuracy: 0.9722 - val_loss: 0.5409 - val_accuracy: 0.9071\n",
      "Epoch 96/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0681 - accuracy: 0.9748 - val_loss: 0.5181 - val_accuracy: 0.8871\n",
      "Epoch 97/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0589 - accuracy: 0.9777 - val_loss: 0.4647 - val_accuracy: 0.9086\n",
      "Epoch 98/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.5748 - val_accuracy: 0.8965\n",
      "Epoch 99/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0641 - accuracy: 0.9785 - val_loss: 0.4372 - val_accuracy: 0.9176\n",
      "Epoch 100/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0527 - accuracy: 0.9812 - val_loss: 0.4881 - val_accuracy: 0.9133\n",
      "Epoch 101/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0624 - accuracy: 0.9782 - val_loss: 0.4953 - val_accuracy: 0.9059\n",
      "Epoch 102/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0659 - accuracy: 0.9757 - val_loss: 0.5168 - val_accuracy: 0.9173\n",
      "Epoch 103/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.7532 - val_accuracy: 0.8929\n",
      "Epoch 104/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0710 - accuracy: 0.9757 - val_loss: 0.6225 - val_accuracy: 0.9047\n",
      "Epoch 105/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0517 - accuracy: 0.9830 - val_loss: 0.4417 - val_accuracy: 0.9294\n",
      "Epoch 106/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0471 - accuracy: 0.9820 - val_loss: 0.6200 - val_accuracy: 0.8980\n",
      "Epoch 107/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0455 - accuracy: 0.9829 - val_loss: 0.6010 - val_accuracy: 0.8988\n",
      "Epoch 108/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0528 - accuracy: 0.9828 - val_loss: 0.5549 - val_accuracy: 0.9216\n",
      "Epoch 109/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0427 - accuracy: 0.9848 - val_loss: 0.6935 - val_accuracy: 0.9055\n",
      "Epoch 110/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0338 - accuracy: 0.9870 - val_loss: 0.5107 - val_accuracy: 0.9102\n",
      "Epoch 111/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0485 - accuracy: 0.9839 - val_loss: 0.5888 - val_accuracy: 0.9020\n",
      "Epoch 112/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.6084 - val_accuracy: 0.9122\n",
      "Epoch 113/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0463 - accuracy: 0.9840 - val_loss: 0.5581 - val_accuracy: 0.9192\n",
      "Epoch 114/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0386 - accuracy: 0.9858 - val_loss: 0.5490 - val_accuracy: 0.9086\n",
      "Epoch 115/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0454 - accuracy: 0.9843 - val_loss: 0.6531 - val_accuracy: 0.8941\n",
      "Epoch 116/250\n",
      "85/85 [==============================] - 16s 184ms/step - loss: 0.0401 - accuracy: 0.9855 - val_loss: 0.5479 - val_accuracy: 0.9212\n",
      "Epoch 117/250\n",
      "85/85 [==============================] - 16s 185ms/step - loss: 0.0558 - accuracy: 0.9844 - val_loss: 0.5099 - val_accuracy: 0.9196\n",
      "Epoch 118/250\n",
      "85/85 [==============================] - 15s 176ms/step - loss: 0.1894 - accuracy: 0.9451 - val_loss: 0.5972 - val_accuracy: 0.8929\n",
      "Epoch 119/250\n",
      "85/85 [==============================] - 16s 186ms/step - loss: 0.0643 - accuracy: 0.9791 - val_loss: 0.5487 - val_accuracy: 0.9094\n",
      "Epoch 120/250\n",
      "85/85 [==============================] - 15s 174ms/step - loss: 0.0554 - accuracy: 0.9804 - val_loss: 0.6509 - val_accuracy: 0.9039\n",
      "Epoch 121/250\n",
      "45/85 [==============>...............] - ETA: 7s - loss: 0.0319 - accuracy: 0.9882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), loss=categorical_squared_hinge, metrics=['accuracy'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m history \u001b[38;5;241m=\u001b[39m LossHistory() \u001b[38;5;66;03m# 创建一个history实例\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_trian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# pred_test = o_vs_r.predict(data_test.reshape(-1, 1))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# pred_test = o_vs_r.predict(data_test)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# pred_ytest = clf.predict((y_test).reshape(1,-1))\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model.save('TestANN.h5')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(\"test Accuracy:\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(f\"{accuracy_score(label_test,pred_test ):.2%}\\n\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m preds_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(data_train, label_trian)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01/250, amsgrad=False), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), loss=categorical_squared_hinge, metrics=['accuracy'])\n",
    "history = LossHistory() # 创建一个history实例\n",
    "\n",
    "model.fit(data_train, label_trian, epochs=250, batch_size=128, verbose=1,\n",
    "            validation_data=(data_test, label_test),callbacks=[history])\n",
    "# pred_test = o_vs_r.predict(data_test.reshape(-1, 1))\n",
    "# pred_test = o_vs_r.predict(data_test)\n",
    "# pred_ytest = clf.predict((y_test).reshape(1,-1))\n",
    "# model.save('TestANN.h5')\n",
    "# print(\"test Accuracy:\")\n",
    "# print(f\"{accuracy_score(label_test,pred_test ):.2%}\\n\")\n",
    "preds_train = model.evaluate(data_train, label_trian)\n",
    "print(\"Train Loss = \" + str(preds_train[0]))\n",
    "print(\"Train Accuracy = \" + str(preds_train[1]))\n",
    "\n",
    "preds_test  = model.evaluate(data_test, label_test)\n",
    "print(\"Test Loss = \" + str(preds_test[0]))\n",
    "print(\"Test Accuracy = \" + str(preds_test[1]))\n",
    "\n",
    "end = time.time()\n",
    "print(\"time:\",end-start)\n",
    "\n",
    "history.loss_plot('epoch')\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c4315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
